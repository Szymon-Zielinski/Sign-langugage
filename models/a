import cv2
import numpy as np

# Załaduj model detekcji ręki (może to być YOLO, Haar Cascade itp.)
hand_detection_model = cv2.CascadeClassifier('path/to/your/hand/detection/model.xml')

# Załaduj model do rozpoznawania liter z użyciem języka migowego
sign_language_model = load_model('path/to/your/sign/language/model.h5')

# Otwórz kamerę internetową
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()

    # Wykonaj detekcję ręki
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    hands = hand_detection_model.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    for (x, y, w, h) in hands:
        # Przycięcie obszaru ręki
        hand_region = frame[y:y + h, x:x + w]

        # Przekształć obszar ręki do formatu wejściowego modelu rozpoznawania liter
        hand_region = cv2.resize(hand_region, (your_input_shape))  # Dopasuj wymiary do modelu

        # Normalizuj dane, jeśli to konieczne (zależnie od tego, jakie dane wejściowe oczekuje model)
        hand_region = hand_region / 255.0  # Przykładowa normalizacja dla obrazów RGB

        # Dodaj dodatkowy wymiar, jeśli model oczekuje partii obrazów
        hand_region = np.expand_dims(hand_region, axis=0)

        # Wykonaj rozpoznawanie liter
        predicted_letter = sign_language_model.predict(hand_region)
        
        # Pobierz indeks klasy z maksymalną pewnością
        predicted_class = np.argmax(predicted_letter)

        # Mapuj indeks klasy na przypisaną literę (na przykład A, B, C...)
        predicted_letter = map_index_to_letter(predicted_class)

        # Wyświetl wyniki na ekranie
        cv2.putText(frame, f"Predicted Letter: {predicted_letter}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow('Sign Language Recognition', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
